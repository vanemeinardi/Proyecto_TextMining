{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25423,"status":"ok","timestamp":1739388941070,"user":{"displayName":"Vanesa Beatriz Meinardi","userId":"05011540024275555334"},"user_tz":180},"id":"MWl16IBXHMhS","outputId":"07b5deb4-9ae7-4c63-c52b-6d2524cf3592"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","'Analisis I'\n"," AutoEvaluaciónProfesoradoMatemática\n","'Curso aprendizaje automático 2023'\n","'Diplo Análisis de datos Ciencias Sociales'\n","'Estadística descriptiva aplicada a la investigación'\n"," Funesil\n"," Grabaciones_Estadistica_descriptiva_aplicada_a_la_investigación\n"," Profesorado\n"," ProyectoUNVM-2023-2024\n","'Rocío doctorado'\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Listar el contenido del directorio principal de Drive\n","!ls \"/content/drive/Shared drives\"\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1027,"status":"ok","timestamp":1734716160849,"user":{"displayName":"Vanesa Beatriz Meinardi","userId":"05011540024275555334"},"user_tz":180},"id":"Vvbyh6YjJ4pM","outputId":"2f13f494-27ad-4188-eb06-5fa7aaf70109"},"outputs":[{"name":"stdout","output_type":"stream","text":["aborto_junio_tweets.csv\t\t\t\t      LDA_decision_tree_metrics.csv\n","aborto_junio_users.csv\t\t\t\t      LDA_decision_tree_results_sorted.csv\n","aborto_junio_users.xlsx\t\t\t\t      LDA_gradient_boosting_metrics_chi.csv\n","adaboost_metrics_chi.csv\t\t\t      LDA_gradient_boosting_metrics.csv\n","adaboost_metrics.csv\t\t\t\t      LDA_logistic_regression_metrics_chi.csv\n","Analisis_sentimiento_comunidades.csv\t\t      LDA_logistic_regression_metrics.csv\n","Analisis_sentimiento_LDA.csv\t\t\t      LDA_logistic_regression_results_sorted.csv\n","baseline_metrics.csv\t\t\t\t      LDA_modelos_chi.csv\n","Caracteristicas_comunidades.csv\t\t\t      LDA_modelos.csv\n","Caracteristicas_LDA.csv\t\t\t\t      LDA_random_forest_metrics_chi.csv\n","Comunidades\t\t\t\t\t      LDA_random_forest_metrics.csv\n","comunidades.csv\t\t\t\t\t      LDA_resultados_adaboost.csv\n","comunidades_modelos_chi.csv\t\t\t      LDA_resultados_gradient_boosting.csv\n","comunidades_modelos.csv\t\t\t\t      LDA_resultados_random_forest.csv\n","comunidades_modelos_pca.csv\t\t\t      LDA_trigramas.csv\n","comunidades_modelos.xlsx\t\t\t      LDA_unigramas_chi.csv\n","comunidades_muestreo\t\t\t\t      LDA_unigramas.csv\n","Comunidades_unigramas_chi.csv\t\t\t      lexico_nrc.csv\n","comunidades_unigramas.csv\t\t\t      logistic_regression_metrics_chi.csv\n","decision_tree_metrics_chi.csv\t\t\t      logistic_regression_metrics.csv\n","decision_tree_metrics.csv\t\t\t      logistic_regression_results_sorted.csv\n","decision_tree_results_sorted.csv\t\t      logistic_regression_results_unigramas.csv\n","df_unigramas.csv\t\t\t\t      muestra_comunidad_ultimo.xlsx\n","GB_best_parametros_unigramas_chi.csv\t\t      muestras_topicos.xlsx\n","GB_best_parametros_variables_contextuales.csv\t      random_forest_metrics_chi.csv\n","gradient_boosting_metrics_chi.csv\t\t      random_forest_metrics.csv\n","gradient_boosting_metrics.csv\t\t\t      resultados_adaboost.csv\n","gradient_boosting_results_unigramas.csv\t\t      resultados_gradient_boosting.csv\n","gradient_boosting_results_variables_contextuales.csv  resultados_grid_search.csv\n","Grafos.csv\t\t\t\t\t      resultados_grid_search_gradient_boosting.csv\n","importancia_caracteristicas_RL.csv\t\t      resultados_random_forest.csv\n","LDA\t\t\t\t\t\t      Trigramas_comunidades.csv\n","LDA_adaboost_metrics_chi.csv\t\t\t      tweets_junio_LDA.csv\n","LDA_adaboost_metrics.csv\t\t\t      tweets_junio_procesado.csv\n","LDA_caracteristicas.csv\t\t\t\t      tweets_junio_recortados.csv\n","LDA_comunidades.csv\t\t\t\t      Variables_contextuales_comunidades.csv\n","LDA_decision_tree_metrics_chi.csv\n"]}],"source":[" #Comprueba las carpetas y archivos\n","!ls \"/content/drive/Shared drives/Rocío doctorado/Proyecto NLP/Datos\"\n"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5767,"status":"ok","timestamp":1739388964422,"user":{"displayName":"Vanesa Beatriz Meinardi","userId":"05011540024275555334"},"user_tz":180},"id":"mOJaUz6QHb8Z","outputId":"ffea3fb4-e89c-4208-c43d-ad9f40282bf4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(169639, 14)"]},"metadata":{},"execution_count":2}],"source":["\n","\n","# Si el archivo aparece en la lista, puedes proceder a leerlo:\n","import pandas as pd\n","ruta_archivo = '/content/drive/Shared drives/Rocío doctorado/Proyecto NLP/Datos/LDA_caracteristicas_baseline.csv'\n","data = pd.read_csv(ruta_archivo)\n","\n","# Muestra las primeras filas del archivo\n","data.shape\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":226,"status":"ok","timestamp":1734716578379,"user":{"displayName":"Vanesa Beatriz Meinardi","userId":"05011540024275555334"},"user_tz":180},"id":"k6AutfMOJH6W","outputId":"e92f6217-fa3f-492d-be44-00ec469c4623"},"outputs":[{"data":{"text/plain":["Index(['postura', 'full_text_cleaned', 'Question Mark', 'Question Mark Count',\n","       'Starts with Question Mark', 'Ends with Mark Count', 'Upper Ratio',\n","       'Exclamation Mark', 'Exclamation Mark Count', 'Ellipsis Occurrence',\n","       'Hashtag Presence', 'Text Length', 'URL Count', 'Quotes'],\n","      dtype='object')"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["data.columns"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":82950,"status":"ok","timestamp":1739389053909,"user":{"displayName":"Vanesa Beatriz Meinardi","userId":"05011540024275555334"},"user_tz":180},"id":"bznZ-pCbLLCC","outputId":"c3b9675e-2aed-4d1d-c252-71e9b9440a4b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting fasttext\n","  Downloading fasttext-0.9.3.tar.gz (73 kB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/73.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m73.4/73.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting pybind11>=2.2 (from fasttext)\n","  Using cached pybind11-2.13.6-py3-none-any.whl.metadata (9.5 kB)\n","Requirement already satisfied: setuptools>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from fasttext) (75.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fasttext) (1.26.4)\n","Using cached pybind11-2.13.6-py3-none-any.whl (243 kB)\n","Building wheels for collected packages: fasttext\n","  Building wheel for fasttext (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for fasttext: filename=fasttext-0.9.3-cp311-cp311-linux_x86_64.whl size=4313474 sha256=1f4be8f04e3e49e37725ddf4838061991339044299cc843fdf30ad2038895f15\n","  Stored in directory: /root/.cache/pip/wheels/65/4f/35/5057db0249224e9ab55a513fa6b79451473ceb7713017823c3\n","Successfully built fasttext\n","Installing collected packages: pybind11, fasttext\n","Successfully installed fasttext-0.9.3 pybind11-2.13.6\n"]}],"source":["!pip install fasttext\n"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10037,"status":"ok","timestamp":1739389961852,"user":{"displayName":"Vanesa Beatriz Meinardi","userId":"05011540024275555334"},"user_tz":180},"id":"bZj6n8BxdgoR","outputId":"81ef4e85-824f-424c-fd9b-d17e3fd8ef2e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Archivos creados: fasttext_train.txt y fasttext_test.txt\n"]}],"source":["from sklearn.model_selection import train_test_split\n","# Separar el conjunto de datos en entrenamiento y prueba\n","data = data.dropna(subset=['postura', 'full_text_cleaned'])\n","\n","data_train, data_test = train_test_split(data, test_size=0.3, random_state=42, stratify=data['postura'])\n","\n","# Crear un archivo de texto para el conjunto de entrenamiento\n","with open(\"fasttext_train.txt\", \"w\") as f_train:\n","    for i, row in data_train.iterrows():\n","        etiqueta = row[\"postura\"]\n","        texto = row[\"full_text_cleaned\"]\n","        f_train.write(f\"__label__{etiqueta} {texto}\\n\")\n","\n","# Crear un archivo de texto para el conjunto de prueba\n","with open(\"fasttext_test.txt\", \"w\") as f_test:\n","    for i, row in data_test.iterrows():\n","        etiqueta = row[\"postura\"]\n","        texto = row[\"full_text_cleaned\"]\n","        f_test.write(f\"__label__{etiqueta} {texto}\\n\")\n","\n","print(\"Archivos creados: fasttext_train.txt y fasttext_test.txt\")"]},{"cell_type":"markdown","metadata":{"id":"nFL41cUrUJrM"},"source":["Entrenamos con el modelo que no sobreajusta"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"dqsymtPCstIv","colab":{"base_uri":"https://localhost:8080/","height":332},"executionInfo":{"status":"error","timestamp":1739390434728,"user_tz":180,"elapsed":468973,"user":{"displayName":"Vanesa Beatriz Meinardi","userId":"05011540024275555334"}},"outputId":"2641c51d-751c-4d7d-8250-a156e3aef9de"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-6-c63e843b0774>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0;31m# Entrenamiento del modelo para cada combinación de hiperparámetros\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mmodelo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_supervised\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"fasttext_train.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepocas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Evaluar en el conjunto de entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/fasttext/FastText.py\u001b[0m in \u001b[0;36mtrain_supervised\u001b[0;34m(*kargs, **kwargs)\u001b[0m\n\u001b[1;32m    558\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_build_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmanually_set_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0mft\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_FastText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 560\u001b[0;31m     \u001b[0mfasttext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    561\u001b[0m     \u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mft\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetArgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mft\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["from sklearn.metrics import f1_score\n","import fasttext\n","import matplotlib.pyplot as plt\n","\n","# Definir los valores de los hiperparámetros a probar\n","epocas_list = [10, 25, 50]  # Número de épocas\n","lr_list = [0.01, 0.05, 0.1]  # Tasa de aprendizaje\n","dim_list = [100, 300, 500]   # Dimensionalidad del vector de palabras\n","\n","# Variables para almacenar los mejores resultados y métricas\n","mejor_f1_validacion = 0\n","mejores_parametros = {}\n","\n","# Variables para almacenar métricas para cada combinación\n","resultados = []\n","\n","# Validación cruzada sobre los hiperparámetros\n","for epocas in epocas_list:\n","    for lr in lr_list:\n","        for dim in dim_list:\n","            # Listas para almacenar métricas de entrenamiento y validación\n","            entrenamiento_f1 = []\n","            validacion_f1 = []\n","\n","            # Entrenamiento del modelo para cada combinación de hiperparámetros\n","            modelo = fasttext.train_supervised(\"fasttext_train.txt\", epoch=epocas, lr=lr, dim=dim)\n","\n","            # Evaluar en el conjunto de entrenamiento\n","            y_train_pred, _ = modelo.predict(data_train['full_text_cleaned'].tolist())\n","            y_train_pred = [label[0].split(\"__\")[-1] for label in y_train_pred]\n","            y_train_true = [str(label) for label in data_train['postura'].tolist()]\n","\n","            # Calcular F1-Score en entrenamiento\n","            f1_train = f1_score(y_train_true, y_train_pred, average='macro')\n","            entrenamiento_f1.append(f1_train)\n","\n","            # Evaluar en el conjunto de validación\n","            y_val_pred, _ = modelo.predict(data_test['full_text_cleaned'].tolist())\n","            y_val_pred = [label[0].split(\"__\")[-1] for label in y_val_pred]\n","            y_val_true = [str(label) for label in data_test['postura'].tolist()]\n","\n","            # Calcular F1-Score en validación\n","            f1_val = f1_score(y_val_true, y_val_pred, average='macro')\n","            validacion_f1.append(f1_val)\n","\n","            # Guardar métricas para esta combinación\n","            resultados.append({\n","                'epoch': epocas,\n","                'lr': lr,\n","                'dim': dim,\n","                'f1_train': f1_train,\n","                'f1_validacion': f1_val\n","            })\n","\n","            # Actualizar mejores parámetros si el F1-Score en validación mejora\n","            if f1_val > mejor_f1_validacion:\n","                mejor_f1_validacion = f1_val\n","                mejores_parametros = {\n","                    'epoch': epocas,\n","                    'lr': lr,\n","                    'dim': dim\n","                }\n","\n","# Mostrar los mejores parámetros encontrados\n","print(\"Mejores hiperparámetros encontrados:\")\n","print(mejores_parametros)\n","print(f\"Mejor F1-Score en Validación: {mejor_f1_validacion:.4f}\")\n","\n","# Visualización de los resultados de F1-Score\n","plt.figure(figsize=(12, 8))\n","\n","# Gráfica de F1-Score para cada combinación de hiperparámetros\n","for result in resultados:\n","    combinacion = f\"{result['epoch']}-{result['lr']}-{result['dim']}\"\n","    plt.scatter(combinacion, result['f1_train'], color='blue', label='F1 Entrenamiento' if combinacion == f\"{epocas_list[0]}-{lr_list[0]}-{dim_list[0]}\" else \"\")\n","    plt.scatter(combinacion, result['f1_validacion'], color='orange', label='F1 Validación' if combinacion == f\"{epocas_list[0]}-{lr_list[0]}-{dim_list[0]}\" else \"\")\n","\n","plt.title('Comparación de F1-Score para Diferentes Combinaciones de Hiperparámetros')\n","plt.xlabel('Combinaciones (epoch-lr-dim)')\n","plt.ylabel('F1-Score')\n","plt.xticks(rotation=45)\n","plt.legend()\n","plt.grid()\n","plt.tight_layout()\n","plt.show()\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":92454,"status":"ok","timestamp":1739390570831,"user":{"displayName":"Vanesa Beatriz Meinardi","userId":"05011540024275555334"},"user_tz":180},"id":"YNwcQc9hUcFH","outputId":"4944980f-1a05-4646-a698-601fd2752ef4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Modelo ajustado para 25 épocas: Precisión Entrenamiento = 0.9914, Recall Entrenamiento = 0.9901, Especificidad Entrenamiento = 0.9942, F1-Score Entrenamiento = 0.9904, ROC AUC Entrenamiento = 0.6074\n","Precisión Validación = 0.9750, Recall Validación = 0.9701, Especificidad Validación = 0.9854, F1-Score Validación = 0.9721, ROC AUC Validación = 0.6008\n"]}],"source":["from sklearn.metrics import f1_score, accuracy_score, recall_score, confusion_matrix, roc_auc_score\n","import fasttext\n","import numpy as np\n","\n","# Variables para almacenar métricas\n","entrenamiento_precision = []\n","validacion_precision = []\n","entrenamiento_recall = []\n","validacion_recall = []\n","entrenamiento_f1 = []  # Para almacenar el F1-Score de entrenamiento\n","validacion_f1 = []     # Para almacenar el F1-Score de validación\n","entrenamiento_auc = []\n","validacion_auc = []\n","\n","# Definir el número de épocas a 25\n","num_epocas = 25\n","\n","# Entrenar el modelo solo para 2 épocas\n","modelo = fasttext.train_supervised(\"fasttext_train.txt\", epoch=num_epocas, lr=0.01, dim=300)\n","\n","# Evaluar el modelo en el conjunto de entrenamiento\n","resultado_train = modelo.test(\"fasttext_train.txt\")\n","\n","# Predecir las etiquetas para el conjunto de entrenamiento\n","y_train_pred, _ = modelo.predict(data_train['full_text_cleaned'].tolist())\n","y_train_pred = [label[0].split(\"__\")[-1] for label in y_train_pred]  # Extraer solo la etiqueta\n","\n","# Obtener las etiquetas verdaderas\n","y_train_true = data_train['postura'].tolist()\n","\n","# Asegurar que ambos son del mismo tipo\n","y_train_true = [str(label) for label in y_train_true]\n","y_train_pred = [str(label) for label in y_train_pred]\n","\n","# Calcular las métricas para el conjunto de entrenamiento\n","entrenamiento_precision.append(resultado_train[1])  # Precisión en entrenamiento\n","entrenamiento_recall.append(recall_score(y_train_true, y_train_pred, average='macro'))\n","entrenamiento_f1.append(f1_score(y_train_true, y_train_pred, average='macro'))\n","\n","# Calcular el ROC AUC (necesita las probabilidades de predicción)\n","# Cambiado el método para obtener las probabilidades\n","y_train_prob = modelo.predict(data_train['full_text_cleaned'].tolist(), k=-1)[1]\n","y_train_prob = np.array([list(map(float, pred)) for pred in y_train_prob])  # Convertir a un array NumPy\n","\n","# Suponiendo que estamos trabajando con un problema binario, selecciona las probabilidades de la clase positiva (1)\n","entrenamiento_auc.append(roc_auc_score(y_train_true, y_train_prob[:, 1]))  # Asegúrate de que esto tiene sentido para tu caso\n","\n","# Evaluar el modelo en el conjunto de validación\n","resultado_val = modelo.test(\"fasttext_test.txt\")\n","\n","# Predecir las etiquetas para el conjunto de validación\n","y_val_pred, _ = modelo.predict(data_test['full_text_cleaned'].tolist())\n","y_val_pred = [label[0].split(\"__\")[-1] for label in y_val_pred]  # Extraer solo la etiqueta\n","\n","# Obtener las etiquetas verdaderas del conjunto de validación\n","y_val_true = data_test['postura'].tolist()\n","\n","# Asegurar que ambos son del mismo tipo\n","y_val_true = [str(label) for label in y_val_true]\n","y_val_pred = [str(label) for label in y_val_pred]\n","\n","# Calcular las métricas para el conjunto de validación\n","validacion_precision.append(resultado_val[1])  # Precisión en validación\n","validacion_recall.append(recall_score(y_val_true, y_val_pred, average='macro'))\n","validacion_f1.append(f1_score(y_val_true, y_val_pred, average='macro'))\n","\n","# Calcular el ROC AUC para el conjunto de validación\n","y_val_prob = modelo.predict(data_test['full_text_cleaned'].tolist(), k=-1)[1]\n","y_val_prob = np.array([list(map(float, pred)) for pred in y_val_prob])  # Convertir a un array NumPy\n","\n","# Suponiendo que estamos trabajando con un problema binario, selecciona las probabilidades de la clase positiva (1)\n","validacion_auc.append(roc_auc_score(y_val_true, y_val_prob[:, 1]))  # Asegúrate de que esto tiene sentido para tu caso\n","\n","# Función para calcular especificidad\n","def specificity(y_true, y_pred):\n","    cm = confusion_matrix(y_true, y_pred)\n","    tn, fp, fn, tp = cm.ravel()\n","    return tn / (tn + fp)\n","\n","# Calcular especificidad\n","especificidad_entrenamiento = specificity(y_train_true, y_train_pred)\n","especificidad_validacion = specificity(y_val_true, y_val_pred)\n","\n","# Imprimir las métricas finales\n","print(f\"Modelo ajustado para 25 épocas: \"\n","      f\"Precisión Entrenamiento = {entrenamiento_precision[-1]:.4f}, \"\n","      f\"Recall Entrenamiento = {entrenamiento_recall[-1]:.4f}, \"\n","      f\"Especificidad Entrenamiento = {especificidad_entrenamiento:.4f}, \"\n","      f\"F1-Score Entrenamiento = {entrenamiento_f1[-1]:.4f}, \"\n","      f\"ROC AUC Entrenamiento = {entrenamiento_auc[-1]:.4f}\")\n","\n","print(f\"Precisión Validación = {validacion_precision[-1]:.4f}, \"\n","      f\"Recall Validación = {validacion_recall[-1]:.4f}, \"\n","      f\"Especificidad Validación = {especificidad_validacion:.4f}, \"\n","      f\"F1-Score Validación = {validacion_f1[-1]:.4f}, \"\n","      f\"ROC AUC Validación = {validacion_auc[-1]:.4f}\")\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyMm+B/5p+MWkAtPRSX3BJLP"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.6"}},"nbformat":4,"nbformat_minor":0}